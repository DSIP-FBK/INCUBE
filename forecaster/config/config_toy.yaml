main:
  name: toy # name of the model
  main_folder: /home/agobbi/Projects/Incube/forecaster/data/outputs #folder in which save the metadata of the model and the weights
  version: 2 # version of the model
  retrain: true #usually you don't want to overwrite a model, but you can force it
  end_point:
    data_url: /home/agobbi/Projects/Incube/forecaster/data/toy.csv # end point for the data. In this version only csv are supported. In the future will be implemented also a connector to the logbook
    parameters: # some parameters required, maybe some more parameters are required for retrieve data from the logbook
      sourceId: "source_id"          # FILL THIS (see delivery task 3.5)
      buildingName: "building_id"    # FILL THIS (see delivery task 3.5)
      spaceName: "spaceName"         # FILL THIS (see delivery task 3.5)
      site: TEST                     # SITE (see delivery task 3.5)
    notification_url: "http://160.40.51.98:8080/api/dbl/api/setEvent" # where send notification after the train is concluded



timeseries:
  enrich_cat: ['dow','hour']         # categorical variable to add to the timeseries
  past_variables: ['rain','temp']    # past covariates
  future_variables: ['rain','temp']  # future known covariates
  transform: 'np.log(500*x)-2'       # you can apply a transformation to the data for removing some spikes
  inv_transform: 'np.exp(x+2)/500'


## in qhat follows leave null the null, they depends on the loaded data. You can change the parameters with  ##CHANGE THIS 
model_configs:
  past_steps: 168              ##CHANGE THIS  168 steps in the past 
  future_steps: 48             ##CHANGE THIS  48 steps in the future 
  quantiles: [0.05,0.5,0.95]   ##CHANGE THIS  it can be [] or 3 values indicating the three quantiles to compute: [0.05,0.5,0.95]--> 90% CI and median

  d_model: 32
  n_head: 4
  hidden_size: 64
  dropout_rate: 0.25
  n_layer_decoder: 2
  use_norm: True
  optim: torch.optim.Adam
  persistence_weight: 0.010
  loss_type: 'l1'
  activation: torch.nn.PReLU
  class_strategy: 'average' 

  #leave these null, depends on the dataset
  past_channels: null
  future_channels: null
  embs: null
  out_channels: null

scheduler_config:
  gamma: 0.1
  step_size: 500

optim_config:
  lr: 0.0005
  weight_decay: 0.01

  
split_params:
  perc_train: 0.75  ##CHANGE THIS 
  perc_valid: 0.2   ##CHANGE THIS 
  shift: 0
  skip_step: 1

  #leave null these
  range_train: null
  range_validation: null
  range_test: null
  starting_point: null
  past_steps: null
  future_steps: null

train_config:
  batch_size: 64
  max_epochs: 4   ##CHANGE THIS 
  num_workers: 0
  auto_lr_find: true
  devices: [0]                   
  seed: 42   
  dirpath: null 